{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uUfDX1i4b-aP"
   },
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tfds.disable_progress_bar()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = tfds.load(\n",
    "    'imdb_reviews',\n",
    "    split='train+test',\n",
    "    as_supervised=True,\n",
    "    batch_size=-1,\n",
    "    shuffle_files=False\n",
    ")\n",
    "reviews, labels = tfds.as_numpy(dataset)\n",
    "print('Total examples:', reviews.shape[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r8VJugMQcFAR",
    "outputId": "dca21220-27d5-4b42-a73d-672264b7bc26"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# import pandas as pd\n",
    "# def show_missing_values(data):\n",
    "#   missing_data = data.isna().sum()\n",
    "#   missing_data_percent = data.isna().sum() / len(data)\n",
    "#   print(\"Duplicated data: \", data.duplicated().sum())\n",
    "#   result = pd.concat([missing_data, missing_data_percent], axis=1)\n",
    "#   result.columns = ['Missing Count', 'Missing Percent']\n",
    "#   return result\n",
    "\n",
    "# show_missing_values(dataset)\n",
    "\n",
    "\"\"\"\n",
    "Not worked because data packed with tuples.\n",
    "\"\"\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xWy4v3WdisYl",
    "outputId": "c89b10b7-cdec-414e-f8d8-dff7c62efa40"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data PreProcessing"
   ],
   "metadata": {
    "id": "q7nZfKXIvkMn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "val_split   = 2500\n",
    "test_split  = 2500\n",
    "train_split = 7500\n",
    "\n",
    "# Separating the negative and positive samples for manual stratification\n",
    "x_positives, y_positives = reviews[labels == 1], labels[labels == 1]\n",
    "x_negatives, y_negatives = reviews[labels == 0], labels[labels == 0]\n",
    "\n",
    "# Creating training, validation and testing splits\n",
    "x_val, y_val = (\n",
    "    tf.concat(\n",
    "        (\n",
    "            x_positives[:val_split], x_negatives[:val_split]\n",
    "\n",
    "        ),\n",
    "        0\n",
    "    ),\n",
    "    tf.concat(\n",
    "        (\n",
    "            y_positives[:val_split], y_negatives[:val_split]\n",
    "        ),\n",
    "        0\n",
    "    )\n",
    ")\n",
    "x_test, y_test = (\n",
    "    tf.concat(\n",
    "        (\n",
    "            x_positives[val_split : val_split + test_split],\n",
    "            x_negatives[val_split : val_split + test_split]\n",
    "        ),\n",
    "        0\n",
    "    ),\n",
    "    tf.concat(\n",
    "        (\n",
    "            y_positives[val_split : val_split + test_split],\n",
    "            y_negatives[val_split : val_split + test_split]\n",
    "        ),\n",
    "        0\n",
    "    ,\n",
    "    )\n",
    ")\n",
    "x_train, y_train = (\n",
    "    tf.concat(\n",
    "        (\n",
    "            x_positives[val_split + test_split : val_split + test_split + train_split],\n",
    "            x_negatives[val_split + test_split : val_split + test_split + train_split]\n",
    "        ),\n",
    "        0\n",
    "    ),\n",
    "    tf.concat(\n",
    "        (\n",
    "            y_positives[val_split + test_split : val_split + test_split + train_split],\n",
    "            y_negatives[val_split + test_split : val_split + test_split + train_split]\n",
    "        ),\n",
    "        0\n",
    "    )\n",
    ")\n",
    "x_pool_positives, y_pool_positives = (\n",
    "    x_positives[val_split + test_split + train_split :],\n",
    "    y_positives[val_split + test_split + train_split :]\n",
    ")\n",
    "x_pool_negatives, y_pool_negatives = (\n",
    "    x_negatives[val_split + test_split + train_split :],\n",
    "    y_negatives[val_split + test_split + train_split :]\n",
    ")\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "val_dataset   = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "test_dataset  = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "pool_negatives = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_pool_negatives, y_pool_negatives)\n",
    ")\n",
    "\n",
    "pool_positives = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_pool_positives, y_pool_positives)\n",
    ")\n",
    "print(f\"Initial training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Testing set size: {len(test_dataset)}\")\n",
    "print(f\"Unlabeled negative set size: {len(pool_negatives)}\")\n",
    "print(f\"Unlabeled positives set size: {len(pool_positives)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7waN5-kacG_Y",
    "outputId": "2db52a2e-f167-41e2-c78d-6668b3bec702"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Standardization and WORD2VEC"
   ],
   "metadata": {
    "id": "fKIm4_t4vnND"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Custom standardization\n",
    "def standardization(data):\n",
    "  lowercase = tf.strings.lower(data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(\n",
    "      stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "  )\n",
    "\n",
    "vectorizer = layers.TextVectorization(3000, standardize=standardization, output_sequence_length=150)\n",
    "vectorizer.adapt(train_dataset.map(lambda x, y: x, num_parallel_calls=tf.data.AUTOTUNE).batch(256))\n",
    "\n",
    "# WORD2VEC\n",
    "def vectorize_text(text, label):\n",
    "  text = vectorizer(text)\n",
    "  return text, label\n",
    "\n",
    "# Vectrize --> train,test, val, pool negatives & positives datasets.\n",
    "train_dataset = train_dataset.map(\n",
    "    vectorize_text, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "pool_negatives = pool_negatives.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "pool_positives = pool_positives.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_dataset.batch(256).map(\n",
    "    vectorize_text, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "test_dataset = test_dataset.batch(256).map(\n",
    "    vectorize_text, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")"
   ],
   "metadata": {
    "id": "8wXlyGhzcPTy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper Functions"
   ],
   "metadata": {
    "id": "vFxz6BV2ht3B"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def append_history(losses, val_losses, accuracy, val_accuracy, history):\n",
    "  losses = losses + history.history['loss']\n",
    "  val_losses = val_losses + history.history['val_loss']\n",
    "  accuracy = accuracy + history.history['binary_accuracy']\n",
    "  val_accuracy = val_accuracy + history.history['val_binary_accuracy']\n",
    "  return losses, val_losses, accuracy, val_accuracy\n",
    "\n",
    "# Plot\n",
    "def plot_history(losses, val_losses, accuracies, val_accuracies):\n",
    "    fig = plt.figure()\n",
    "    fig.set_facecolor('black')\n",
    "    plt.plot(losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.legend([\"train_loss\", \"val_loss\"])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(accuracies)\n",
    "    plt.plot(val_accuracies)\n",
    "    plt.legend([\"train_accuracy\", \"val_accuracy\"])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "id": "rfnk5YEMgvvf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_model():\n",
    "  model = keras.models.Sequential(\n",
    "      [\n",
    "          layers.Input(shape=(150,)),\n",
    "          layers.Embedding(input_dim=3000, output_dim=128),\n",
    "          layers.Bidirectional(layers.LSTM(32, return_sequences=True)),\n",
    "          layers.GlobalMaxPooling1D(),\n",
    "          layers.Dense(20, activation='relu'),\n",
    "          layers.Dropout(0.25),\n",
    "          layers.Dense(1, activation='sigmoid')\n",
    "      ]\n",
    "  )\n",
    "  model.summary()\n",
    "  return model"
   ],
   "metadata": {
    "id": "cKvy-XkJkBss"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Model"
   ],
   "metadata": {
    "id": "at1h7XiIvscA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train_full_model(full_train_dataset, val_dataset, test_dataset):\n",
    "  model = create_model()\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer='rmsprop',\n",
    "      metrics=[\n",
    "        keras.metrics.BinaryAccuracy(),\n",
    "        keras.metrics.FalseNegatives(),\n",
    "        keras.metrics.FalsePositives(),\n",
    "      ],\n",
    "  )\n",
    "\n",
    "  history = model.fit(\n",
    "      full_train_dataset.batch(256),\n",
    "      epochs=20, # Train round\n",
    "      validation_data=val_dataset,\n",
    "      callbacks=[\n",
    "          keras.callbacks.EarlyStopping(patience=4),\n",
    "          keras.callbacks.ModelCheckpoint( # Save best turn in model.\n",
    "              'FullModelCheckpoint.h5', save_best_only=True\n",
    "          ),\n",
    "      ],\n",
    "  )\n",
    "\n",
    "\n",
    "  model = keras.models.load_model('FullModelCheckpoint.h5')\n",
    "  print('-' * 100)\n",
    "  print(\"test set evaluation: \", model.evaluate(test_dataset, return_dict=True))\n",
    "  print('-' * 100)\n",
    "  return model\n",
    "\n",
    "full_train_dataset=(\n",
    "    train_dataset.concatenate(pool_positives).concatenate(pool_negatives).cache().shuffle(20000)\n",
    ")\n",
    "\n",
    "full_dataset_model = train_full_model(full_train_dataset, val_dataset, test_dataset)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Tg2fXXplShn",
    "outputId": "bf149d3b-070f-4ed3-c546-a4652dbdae47"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "fWA184W6uTbq"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1) False Negatives / All False Predictions\n",
    "\n",
    "2) False Positives / All False Predictions"
   ],
   "metadata": {
    "id": "NFRIslVHuap2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training via Active Learning"
   ],
   "metadata": {
    "id": "aC4C3ngOvcJj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train_active_learning_models(\n",
    "    train_dataset,\n",
    "    pool_negatives,\n",
    "    pool_positives,\n",
    "    val_dataset,\n",
    "    test_dataset,\n",
    "    num_iterations=3,\n",
    "    sampling_size=5000\n",
    "):\n",
    "    losses, val_losses, accuracies, val_accuracies = [], [], [], []\n",
    "\n",
    "    model = create_model()\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='rmsprop',\n",
    "        metrics=[\n",
    "        keras.metrics.BinaryAccuracy(),\n",
    "        keras.metrics.FalseNegatives(),\n",
    "        keras.metrics.FalsePositives(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        'AL_Model.h5', save_best_only=True\n",
    "    )\n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience=4)\n",
    "\n",
    "    print(f\"Starting to train with {len(train_dataset)} samples\")\n",
    "\n",
    "    history = model.fit(\n",
    "    full_train_dataset.shuffle(20000).batch(256),\n",
    "    epochs=20, # Train round\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[\n",
    "        checkpoint,\n",
    "        early_stopping\n",
    "      ],\n",
    "    )\n",
    "\n",
    "    losses, val_losses, accuracies, val_accuracies = append_history(\n",
    "         losses, val_losses, accuracies, val_accuracies, history\n",
    "    )\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "      predictions = model.predict(test_dataset)\n",
    "\n",
    "      rounded = tf.where(tf.greater(predictions, 0.5), 1, 0)\n",
    "\n",
    "      _, _, false_negatives, false_positives = model.evaluate(test_dataset)\n",
    "\n",
    "      print(\"-\" * 100)\n",
    "      print(\n",
    "          f\"Number of zeros incorrectly classified: {false_negatives}, Number of ones incorrectly classified: {false_positives}\"\n",
    "      )\n",
    "\n",
    "      if false_negatives != 0 and false_positives !=0:\n",
    "        total = false_positives + false_positives\n",
    "        sample_ratio_ones, sample_ratio_zeros = (\n",
    "            false_positives / total,\n",
    "            false_negatives / total,\n",
    "        )\n",
    "\n",
    "      else:\n",
    "        sample_ratio_ones, sample_ratio_zeros = 0.5, 0.5\n",
    "      print(\n",
    "          f\"Sample ratio for positives: {sample_ratio_ones}, Sample ratio for negatives:{sample_ratio_zeros}\"\n",
    "          )\n",
    "      sampled_dataset = pool_negatives.take(\n",
    "          int(sample_ratio_zeros * sampling_size)\n",
    "      ).concatenate(pool_positives.take(int(sample_ratio_ones * sampling_size)))\n",
    "\n",
    "      pool_negatives = pool_negatives.skip(int(sample_ratio_zeros * sampling_size))\n",
    "      pool_positives = pool_positives.skip(int(sample_ratio_ones * sampling_size))\n",
    "\n",
    "      train_dataset = train_dataset.concatenate(sampled_dataset).prefetch(tf.data.AUTOTUNE)\n",
    "      print(f\"Starting training with {len(train_dataset)} samples\")\n",
    "      print(\"-\" * 100)\n",
    "\n",
    "      model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            optimizer=\"rmsprop\",\n",
    "            metrics=[\n",
    "                keras.metrics.BinaryAccuracy(),\n",
    "                keras.metrics.FalseNegatives(),\n",
    "                keras.metrics.FalsePositives(),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "      history = model.fit(\n",
    "          train_dataset.cache().shuffle(20000).batch(256),\n",
    "          validation_data=val_dataset,\n",
    "          epochs=20,\n",
    "          callbacks=[\n",
    "              checkpoint,\n",
    "              keras.callbacks.EarlyStopping(patience=4, verbose=1),\n",
    "          ],\n",
    "      )\n",
    "      losses, val_losses, accuracies, val_accuracies = append_history(\n",
    "            losses, val_losses, accuracies, val_accuracies, history\n",
    "        )\n",
    "\n",
    "        # Loading the best model from this training loop\n",
    "      model = keras.models.load_model(\"AL_Model.h5\")\n",
    "    plot_history(losses, val_losses, accuracies, val_accuracies)\n",
    "    print(\"-\" * 100)\n",
    "    print(\n",
    "        \"Test set evaluation: \",\n",
    "        model.evaluate(test_dataset, verbose=0, return_dict=True),\n",
    "    )\n",
    "    print(\"-\" * 100)\n",
    "    return model\n",
    "active_learning_model = train_active_learning_models(\n",
    "    train_dataset, pool_negatives, pool_positives, val_dataset, test_dataset\n",
    ")"
   ],
   "metadata": {
    "id": "-UZ9NcaXuxtc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "3c24784b-c6a9-4480-bb6d-83f840ca7bd6"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
